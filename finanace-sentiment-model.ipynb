{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 57/57 [00:08<00:00,  7.11it/s, loss=1.08]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Average Loss: 1.1177\n",
      "Validation Accuracy after Epoch 1: 60.18%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 57/57 [00:08<00:00,  6.86it/s, loss=1.04]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Average Loss: 1.0967\n",
      "Validation Accuracy after Epoch 2: 55.75%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 57/57 [00:07<00:00,  7.26it/s, loss=0.649]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Average Loss: 0.9281\n",
      "Validation Accuracy after Epoch 3: 66.37%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 57/57 [00:07<00:00,  7.27it/s, loss=0.617]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Average Loss: 0.8433\n",
      "Validation Accuracy after Epoch 4: 64.60%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 57/57 [00:08<00:00,  7.06it/s, loss=0.894]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20], Average Loss: 0.7558\n",
      "Validation Accuracy after Epoch 5: 53.10%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 57/57 [00:08<00:00,  6.70it/s, loss=0.664]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20], Average Loss: 0.6848\n",
      "Validation Accuracy after Epoch 6: 54.42%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 57/57 [00:07<00:00,  7.19it/s, loss=0.638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20], Average Loss: 0.6748\n",
      "Validation Accuracy after Epoch 7: 63.27%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 57/57 [00:07<00:00,  7.27it/s, loss=0.506]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20], Average Loss: 0.6335\n",
      "Validation Accuracy after Epoch 8: 60.18%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 57/57 [00:08<00:00,  7.01it/s, loss=0.563]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20], Average Loss: 0.5614\n",
      "Validation Accuracy after Epoch 9: 60.62%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 57/57 [00:07<00:00,  7.23it/s, loss=0.88] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20], Average Loss: 0.5389\n",
      "Validation Accuracy after Epoch 10: 51.33%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 57/57 [00:08<00:00,  7.07it/s, loss=0.348]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20], Average Loss: 0.4642\n",
      "Validation Accuracy after Epoch 11: 58.41%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 57/57 [00:08<00:00,  7.09it/s, loss=0.19] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20], Average Loss: 0.4161\n",
      "Validation Accuracy after Epoch 12: 68.14%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 57/57 [00:07<00:00,  7.22it/s, loss=0.287] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20], Average Loss: 0.2953\n",
      "Validation Accuracy after Epoch 13: 62.39%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 57/57 [00:07<00:00,  7.16it/s, loss=0.613] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20], Average Loss: 0.2629\n",
      "Validation Accuracy after Epoch 14: 70.80%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 57/57 [00:07<00:00,  7.15it/s, loss=0.429] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20], Average Loss: 0.2374\n",
      "Validation Accuracy after Epoch 15: 68.58%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 57/57 [00:08<00:00,  6.98it/s, loss=0.617] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20], Average Loss: 0.2184\n",
      "Validation Accuracy after Epoch 16: 63.27%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 57/57 [00:08<00:00,  7.08it/s, loss=0.342] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20], Average Loss: 0.1649\n",
      "Validation Accuracy after Epoch 17: 67.70%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 57/57 [00:07<00:00,  7.20it/s, loss=0.0133]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20], Average Loss: 0.1173\n",
      "Validation Accuracy after Epoch 18: 61.50%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 57/57 [00:07<00:00,  7.20it/s, loss=0.155]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20], Average Loss: 0.1047\n",
      "Validation Accuracy after Epoch 19: 63.27%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 57/57 [00:07<00:00,  7.19it/s, loss=0.0983] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20], Average Loss: 0.1030\n",
      "Validation Accuracy after Epoch 20: 61.50%\n",
      "\n",
      "Test Accuracy: 64.76%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.74      0.60        31\n",
      "     neutral       1.00      0.56      0.72       139\n",
      "    positive       0.45      0.81      0.57        57\n",
      "\n",
      "    accuracy                           0.65       227\n",
      "   macro avg       0.65      0.70      0.63       227\n",
      "weighted avg       0.79      0.65      0.67       227\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator, GloVe\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_DIM = 100  # GloVe 임베딩 차원과 일치시킴\n",
    "NUM_CLASSES = 3  # 클래스 수 (negative, neutral, positive)\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 5e-4\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "MAX_SEQ_LEN = 256\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 데이터셋 로드\n",
    "dataset = load_dataset('financial_phrasebank', 'sentences_allagree')\n",
    "\n",
    "# 데이터프레임으로 변환\n",
    "data = dataset['train'].to_pandas()\n",
    "\n",
    "# 클래스 이름 가져오기\n",
    "label_names = dataset['train'].features['label'].names\n",
    "\n",
    "# 데이터 분할\n",
    "train_data, temp_data = train_test_split(\n",
    "    data, test_size=0.2, stratify=data['label'], random_state=42)\n",
    "val_data, test_data = train_test_split(\n",
    "    temp_data, test_size=0.5, stratify=temp_data['label'], random_state=42)\n",
    "\n",
    "# 어휘 사전 구축\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for sentence in data_iter:\n",
    "        yield tokenizer(sentence)\n",
    "\n",
    "vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(train_data['sentence']), \n",
    "    max_tokens=MAX_VOCAB_SIZE, \n",
    "    specials=['<pad>', '<unk>']\n",
    ")\n",
    "vocab.set_default_index(vocab['<unk>'])\n",
    "\n",
    "# 사전 학습된 GloVe 임베딩 로드\n",
    "glove = GloVe(name='6B', dim=EMBEDDING_DIM)\n",
    "\n",
    "# 임베딩 매트릭스 생성\n",
    "embedding_matrix = torch.zeros(len(vocab), EMBEDDING_DIM)\n",
    "for i, token in enumerate(vocab.get_itos()):\n",
    "    if token in glove.stoi:\n",
    "        embedding_matrix[i] = glove[token]\n",
    "    else:\n",
    "        embedding_matrix[i] = torch.randn(EMBEDDING_DIM)\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class FinancialPhraseBankDataset(Dataset):\n",
    "    def __init__(self, data, vocab, tokenizer):\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.data.loc[idx, 'sentence']\n",
    "        label = self.data.loc[idx, 'label']\n",
    "        tokens = self.tokenizer(sentence)\n",
    "        token_ids = [self.vocab[token] for token in tokens]\n",
    "        # 시퀀스 길이 조정 및 패딩\n",
    "        if len(token_ids) > MAX_SEQ_LEN:\n",
    "            token_ids = token_ids[:MAX_SEQ_LEN]\n",
    "        else:\n",
    "            token_ids += [self.vocab['<pad>']] * (MAX_SEQ_LEN - len(token_ids))\n",
    "        text_tensor = torch.tensor(token_ids, dtype=torch.long)\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
    "        return text_tensor, label_tensor\n",
    "\n",
    "# 데이터셋 및 데이터 로더 생성\n",
    "train_dataset = FinancialPhraseBankDataset(train_data, vocab, tokenizer)\n",
    "val_dataset = FinancialPhraseBankDataset(val_data, vocab, tokenizer)\n",
    "test_dataset = FinancialPhraseBankDataset(test_data, vocab, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 클래스 가중치 계산\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced', classes=np.unique(train_data['label']), y=train_data['label']\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# 모델 정의\n",
    "class CNNTransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_classes, embedding_matrix):\n",
    "        super(CNNTransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            embedding_matrix, freeze=False, padding_idx=vocab['<pad>']\n",
    "        )\n",
    "        self.position_embedding = nn.Embedding(MAX_SEQ_LEN, embedding_dim)\n",
    "\n",
    "        # CNN 인코더\n",
    "        self.cnn_encoder = nn.Conv1d(\n",
    "            embedding_dim, embedding_dim, kernel_size=3, padding=1, stride=2\n",
    "        )\n",
    "        self.cnn_encoder_residual = nn.Conv1d(\n",
    "            embedding_dim, embedding_dim, kernel_size=1, stride=2\n",
    "        )\n",
    "\n",
    "        # 트랜스포머 인코더 레이어\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim, nhead=4, dropout=0.1, activation='relu', batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer, num_layers=1, norm=nn.LayerNorm(embedding_dim)\n",
    "        )\n",
    "\n",
    "        # CNN 디코더\n",
    "        self.cnn_decoder = nn.ConvTranspose1d(\n",
    "            embedding_dim, embedding_dim, kernel_size=3, padding=1, stride=2, output_padding=1\n",
    "        )\n",
    "        self.cnn_decoder_residual = nn.ConvTranspose1d(\n",
    "            embedding_dim, embedding_dim, kernel_size=1, stride=2, output_padding=1\n",
    "        )\n",
    "\n",
    "        # 출력 레이어\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        x = self.embedding(x)\n",
    "        positions = torch.arange(0, seq_len).unsqueeze(0).expand(batch_size, seq_len).to(device)\n",
    "        x = x + self.position_embedding(positions)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # CNN 인코더와 잔차 연결\n",
    "        residual = self.cnn_encoder_residual(x)\n",
    "        x = self.cnn_encoder(x)\n",
    "        x = nn.ReLU()(x + residual)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # 패딩 마스크 생성\n",
    "        src_key_padding_mask = (x.abs().sum(dim=2) == 0)\n",
    "        # 트랜스포머 인코더\n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        # CNN 디코더와 잔차 연결\n",
    "        residual = self.cnn_decoder_residual(x)\n",
    "        x = self.cnn_decoder(x)\n",
    "        x = nn.ReLU()(x + residual)\n",
    "        # 글로벌 평균 풀링\n",
    "        x = x.mean(dim=2)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "# 모델 초기화\n",
    "model = CNNTransformerModel(len(vocab), EMBEDDING_DIM, NUM_CLASSES, embedding_matrix).to(device)\n",
    "\n",
    "# 손실 함수와 옵티마이저 설정\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    for texts, labels in progress_bar:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 검증 데이터로 평가\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in val_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            outputs = model(texts)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_accuracy = correct / total\n",
    "    print(f\"Validation Accuracy after Epoch {epoch+1}: {val_accuracy * 100:.2f}%\\n\")\n",
    "\n",
    "# 테스트 데이터로 평가\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for texts, labels in test_loader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        outputs = model(texts)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "test_accuracy = correct / total\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# 분류 보고서 출력\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=label_names))\n",
    "\n",
    "torch.save(model.state_dict(), 'cnn_transformer_model-finance.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''\n",
    "결과 분석\n",
    "1.1 정확도 및 손실 값\n",
    "최종 테스트 정확도: 약 64.76%\n",
    "에포크별 손실 값: 초기에는 높은 손실 값에서 시작하여 에포크가 진행됨에 따라 손실이 감소하는 추세를 보입니다.\n",
    "검증 정확도: 에포크마다 변동이 심하며, 꾸준히 상승하지 않고 불안정한 모습을 보입니다.\n",
    "1.2 분류 보고서\n",
    "Negative 클래스\n",
    "정밀도(Precision): 0.50\n",
    "재현율(Recall): 0.74\n",
    "F1-스코어: 0.60\n",
    "Neutral 클래스\n",
    "정밀도: 1.00\n",
    "재현율: 0.56\n",
    "F1-스코어: 0.72\n",
    "Positive 클래스\n",
    "정밀도: 0.45\n",
    "재현율: 0.81\n",
    "F1-스코어: 0.57\n",
    "1.3 주요 문제점\n",
    "클래스 불균형: Neutral 클래스의 지원(Support) 수가 139로 다른 클래스에 비해 월등히 많습니다.\n",
    "Neutral 클래스의 정밀도는 높지만 재현율이 낮음: 이는 모델이 Neutral로 예측한 것 중 실제로 맞은 비율은 높지만, 실제 Neutral인 것들을 많이 놓치고 있다는 의미입니다.\n",
    "Positive 클래스의 재현율이 높지만 정밀도가 낮음: 모델이 Positive로 예측한 것 중 실제로 맞은 비율은 낮지만, 실제 Positive인 것들을 많이 찾아낸다는 의미입니다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence: The company's profits have increased significantly this quarter.\n",
      "Predicted Sentiment: negative\n",
      "Probabilities: Negative 93.08%, Neutral 0.00%, Positive 6.92%\n"
     ]
    }
   ],
   "source": [
    "# 예측 함수 정의\n",
    "def predict(text):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tokens = tokenizer(text)\n",
    "        token_ids = [vocab[token] for token in tokens]\n",
    "        if len(token_ids) > MAX_SEQ_LEN:\n",
    "            token_ids = token_ids[:MAX_SEQ_LEN]\n",
    "        else:\n",
    "            token_ids += [vocab['<pad>']] * (MAX_SEQ_LEN - len(token_ids))\n",
    "        text_tensor = torch.tensor(token_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "        outputs = model(text_tensor)\n",
    "        probabilities = nn.functional.softmax(outputs, dim=1)\n",
    "        predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "        class_names = label_names\n",
    "        return class_names[predicted_class], probabilities.squeeze().cpu().numpy()\n",
    "\n",
    "# 예시 문장 예측\n",
    "sample_text = \"The company's profits have increased significantly this quarter.\"\n",
    "label, probs = predict(sample_text)\n",
    "print(f\"\\nSentence: {sample_text}\")\n",
    "print(f\"Predicted Sentiment: {label}\")\n",
    "print(f\"Probabilities: Negative {probs[0]*100:.2f}%, Neutral {probs[1]*100:.2f}%, Positive {probs[2]*100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For the last quarter of 2010 , Componenta 's n...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In the third quarter of 2010 , net sales incre...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Operating profit rose to EUR 13.1 mn from EUR ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Operating profit totalled EUR 21.1 mn , up fro...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2259</th>\n",
       "      <td>Operating result for the 12-month period decre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260</th>\n",
       "      <td>HELSINKI Thomson Financial - Shares in Cargote...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>LONDON MarketWatch -- Share prices ended lower...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>Operating profit fell to EUR 35.4 mn from EUR ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>Sales in Finland decreased by 10.5 % in Januar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2264 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sentence  label\n",
       "0     According to Gran , the company has no plans t...      1\n",
       "1     For the last quarter of 2010 , Componenta 's n...      2\n",
       "2     In the third quarter of 2010 , net sales incre...      2\n",
       "3     Operating profit rose to EUR 13.1 mn from EUR ...      2\n",
       "4     Operating profit totalled EUR 21.1 mn , up fro...      2\n",
       "...                                                 ...    ...\n",
       "2259  Operating result for the 12-month period decre...      0\n",
       "2260  HELSINKI Thomson Financial - Shares in Cargote...      0\n",
       "2261  LONDON MarketWatch -- Share prices ended lower...      0\n",
       "2262  Operating profit fell to EUR 35.4 mn from EUR ...      0\n",
       "2263  Sales in Finland decreased by 10.5 % in Januar...      0\n",
       "\n",
       "[2264 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn-transformer-hybrid-model-fa1p7jlT-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
