{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터 레이블 분포: Counter({1: 12500, 2: 12500})\n",
      "테스트 데이터 레이블 분포: Counter({1: 12500, 2: 12500})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/petersong/Library/Caches/pypoetry/virtualenvs/cnn-transformer-hybrid-model-fa1p7jlT-py3.11/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "에포크 1/5: 100%|██████████| 313/313 [07:07<00:00,  1.37s/it, loss=9.78e-6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 [1/5], 평균 손실: 0.0038\n",
      "에포크 1 후 검증 정확도: 100.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "에포크 2/5: 100%|██████████| 313/313 [07:12<00:00,  1.38s/it, loss=1.06e-5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 [2/5], 평균 손실: 0.0000\n",
      "에포크 2 후 검증 정확도: 100.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "에포크 3/5: 100%|██████████| 313/313 [07:04<00:00,  1.36s/it, loss=3.8e-7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 [3/5], 평균 손실: 0.0000\n",
      "에포크 3 후 검증 정확도: 100.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "에포크 4/5: 100%|██████████| 313/313 [07:15<00:00,  1.39s/it, loss=2.15e-6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 [4/5], 평균 손실: 0.0000\n",
      "에포크 4 후 검증 정확도: 100.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "에포크 5/5: 100%|██████████| 313/313 [07:11<00:00,  1.38s/it, loss=2.05e-7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 [5/5], 평균 손실: 0.0000\n",
      "에포크 5 후 검증 정확도: 100.00%\n",
      "\n",
      "테스트 정확도: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from tqdm import tqdm  # 학습 진행 상황을 보여주기 위한 라이브러리\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "BATCH_SIZE = 64\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_CLASSES = 2\n",
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE = 1e-3  # 학습 안정성을 위해 학습률을 낮춤\n",
    "MAX_VOCAB_SIZE = 20000  # 어휘 사전 크기 증가\n",
    "MAX_SEQ_LEN = 512  # 시퀀스 길이 조정\n",
    "\n",
    "# 디바이스 설정 (GPU 사용 가능 시 사용)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 데이터 준비\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for label, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# 어휘 사전 구축 (훈련 데이터만 사용)\n",
    "train_iter = IMDB(split='train')\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), max_tokens=MAX_VOCAB_SIZE, specials=[\"<pad>\", \"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# 데이터 로드\n",
    "train_data = list(IMDB(split='train'))\n",
    "test_data = list(IMDB(split='test'))\n",
    "\n",
    "# 레이블 분포 확인\n",
    "train_labels = [label for label, _ in train_data]\n",
    "test_labels = [label for label, _ in test_data]\n",
    "\n",
    "print(\"훈련 데이터 레이블 분포:\", Counter(train_labels))\n",
    "print(\"테스트 데이터 레이블 분포:\", Counter(test_labels))\n",
    "\n",
    "# 훈련 데이터와 검증 데이터로 분할\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42, stratify=train_labels)\n",
    "\n",
    "def text_pipeline(text):\n",
    "    tokens = tokenizer(text)\n",
    "    token_ids = [vocab[token] for token in tokens]\n",
    "    if len(token_ids) > MAX_SEQ_LEN:\n",
    "        token_ids = token_ids[:MAX_SEQ_LEN]\n",
    "    else:\n",
    "        token_ids += [vocab[\"<pad>\"]] * (MAX_SEQ_LEN - len(token_ids))\n",
    "    return torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "def label_pipeline(label):\n",
    "    return torch.tensor(1 if label == 'pos' else 0, dtype=torch.long)\n",
    "\n",
    "class IMDBDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = []\n",
    "        for label, text in data:\n",
    "            try:\n",
    "                text_tensor = text_pipeline(text)\n",
    "                label_tensor = label_pipeline(label)\n",
    "                self.data.append((text_tensor, label_tensor))\n",
    "            except Exception as e:\n",
    "                print(f\"데이터 처리 오류: {e}\")\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "train_dataset = IMDBDataset(train_data)\n",
    "val_dataset = IMDBDataset(val_data)\n",
    "test_dataset = IMDBDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 모델 정의\n",
    "class CNNTransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_classes):\n",
    "        super(CNNTransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab[\"<pad>\"])\n",
    "        self.position_embedding = nn.Embedding(MAX_SEQ_LEN, embedding_dim)\n",
    "\n",
    "        # CNN 인코더\n",
    "        self.cnn_encoder = nn.Conv1d(in_channels=embedding_dim, out_channels=embedding_dim, kernel_size=3, padding=1, stride=2)\n",
    "        self.cnn_encoder_residual = nn.Conv1d(embedding_dim, embedding_dim, kernel_size=1, stride=2)\n",
    "\n",
    "        # 트랜스포머 인코더 레이어\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=8, dropout=0.1, activation='relu')\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2, norm=nn.LayerNorm(embedding_dim))\n",
    "\n",
    "        # CNN 디코더\n",
    "        self.cnn_decoder = nn.ConvTranspose1d(in_channels=embedding_dim, out_channels=embedding_dim, kernel_size=3, padding=1, stride=2, output_padding=1)\n",
    "        self.cnn_decoder_residual = nn.ConvTranspose1d(embedding_dim, embedding_dim, kernel_size=1, stride=2, output_padding=1)\n",
    "\n",
    "        # 출력 레이어\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        # 임베딩 및 포지셔널 인코딩 추가\n",
    "        x = self.embedding(x)  # [batch_size, seq_len, embedding_dim]\n",
    "        positions = torch.arange(0, seq_len).unsqueeze(0).expand(batch_size, seq_len).to(device)\n",
    "        x = x + self.position_embedding(positions)\n",
    "        x = x.permute(0, 2, 1)  # [batch_size, embedding_dim, seq_len]\n",
    "\n",
    "        # CNN 인코더와 잔차 연결\n",
    "        residual = self.cnn_encoder_residual(x)\n",
    "        x = self.cnn_encoder(x)\n",
    "        x = nn.ReLU()(x + residual)\n",
    "\n",
    "        x = x.permute(2, 0, 1)  # [seq_len', batch_size, embedding_dim]\n",
    "\n",
    "        # 패딩 마스크 생성\n",
    "        src_key_padding_mask = (x.abs().sum(dim=2) == 0).transpose(0, 1)\n",
    "\n",
    "        # 트랜스포머 인코더\n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        x = x.permute(1, 2, 0)  # [batch_size, embedding_dim, seq_len']\n",
    "\n",
    "        # CNN 디코더와 잔차 연결\n",
    "        residual = self.cnn_decoder_residual(x)\n",
    "        x = self.cnn_decoder(x)\n",
    "        x = nn.ReLU()(x + residual)\n",
    "\n",
    "        # 글로벌 평균 풀링\n",
    "        x = x.mean(dim=2)  # [batch_size, embedding_dim]\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc(x)  # [batch_size, num_classes]\n",
    "        return logits\n",
    "\n",
    "# 모델 초기화\n",
    "model = CNNTransformerModel(len(vocab), EMBEDDING_DIM, NUM_CLASSES).to(device)\n",
    "\n",
    "# 손실 함수와 옵티마이저 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"에포크 {epoch+1}/{NUM_EPOCHS}\")\n",
    "    for texts, labels in progress_bar:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"에포크 [{epoch+1}/{NUM_EPOCHS}], 평균 손실: {avg_loss:.4f}\")\n",
    "\n",
    "    # 검증 데이터로 평가\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for texts, labels in val_loader:\n",
    "            texts, labels = texts.to(device), labels.to(device)\n",
    "            outputs = model(texts)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    val_accuracy = correct / total\n",
    "    print(f\"에포크 {epoch+1} 후 검증 정확도: {val_accuracy * 100:.2f}%\\n\")\n",
    "\n",
    "# 테스트 데이터로 최종 평가\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for texts, labels in test_loader:\n",
    "        texts, labels = texts.to(device), labels.to(device)\n",
    "        outputs = model(texts)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "test_accuracy = correct / total\n",
    "print(f\"테스트 정확도: {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "\n",
    "# 모델 저장\n",
    "torch.save(model.state_dict(), 'cnn_transformer_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 초기화 및 가중치 로드\n",
    "model = CNNTransformerModel(len(vocab), EMBEDDING_DIM, NUM_CLASSES).to(device)\n",
    "model.load_state_dict(torch.load('cnn_transformer_model.pth', map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# 예측 함수\n",
    "def predict(text):\n",
    "    with torch.no_grad():\n",
    "        text_tensor = text_pipeline(text).unsqueeze(0).to(device)\n",
    "        outputs = model(text_tensor)\n",
    "        probabilities = nn.functional.softmax(outputs, dim=1)\n",
    "        predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "        class_names = ['Negative', 'Positive']\n",
    "        return class_names[predicted_class], probabilities.squeeze().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"the movie was good\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 결과: Negative\n",
      "확률 분포: Negative 100.00%, Positive 0.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label, probs = predict(user_input)\n",
    "print(f\"예측 결과: {label}\")\n",
    "print(f\"확률 분포: Negative {probs[0]*100:.2f}%, Positive {probs[1]*100:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn-transformer-hybrid-model-fa1p7jlT-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
