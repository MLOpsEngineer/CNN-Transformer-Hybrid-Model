{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/petersong/Library/Caches/pypoetry/virtualenvs/cnn-transformer-hybrid-model-fa1p7jlT-py3.11/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.0022\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# 하이퍼파라미터 설정\n",
    "BATCH_SIZE = 64\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_CLASSES = 2\n",
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE = 0.001\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "MAX_SEQ_LEN = 500\n",
    "\n",
    "# 데이터 준비\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for label, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# IMDB 데이터셋 로드\n",
    "train_iter = IMDB(split='train')\n",
    "test_iter = IMDB(split='test')\n",
    "\n",
    "# 어휘 사전 구축\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), max_tokens=MAX_VOCAB_SIZE, specials=[\"<pad>\", \"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "\n",
    "# 데이터셋을 다시 로드하여 사용 (중요)\n",
    "train_iter = IMDB(split='train')\n",
    "test_iter = IMDB(split='test')\n",
    "\n",
    "def text_pipeline(x):\n",
    "    tokens = tokenizer(x)\n",
    "    token_ids = [vocab[token] for token in tokens]\n",
    "    # 시퀀스 길이 조정\n",
    "    if len(token_ids) > MAX_SEQ_LEN:\n",
    "        token_ids = token_ids[:MAX_SEQ_LEN]\n",
    "    else:\n",
    "        token_ids += [vocab[\"<pad>\"]] * (MAX_SEQ_LEN - len(token_ids))\n",
    "    return token_ids\n",
    "\n",
    "def label_pipeline(x):\n",
    "    return 1 if x == 'pos' else 0\n",
    "\n",
    "# 데이터셋 생성\n",
    "class IMDBDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_iter):\n",
    "        self.data = []\n",
    "        for label, text in data_iter:\n",
    "            self.data.append((torch.tensor(text_pipeline(text), dtype=torch.long),\n",
    "                              label_pipeline(label)))\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "train_dataset = IMDBDataset(IMDB(split='train'))\n",
    "test_dataset = IMDBDataset(IMDB(split='test'))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# 모델 정의\n",
    "class CNNTransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
    "        super(CNNTransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # CNN 인코더\n",
    "        self.cnn_encoder = nn.Conv1d(in_channels=embedding_dim, out_channels=embedding_dim, kernel_size=5, padding=2, stride=2)\n",
    "\n",
    "        # 트랜스포머 인코더 레이어\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=8)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "\n",
    "        # CNN 디코더\n",
    "        self.cnn_decoder = nn.ConvTranspose1d(in_channels=embedding_dim, out_channels=embedding_dim, kernel_size=5, padding=2, stride=2, output_padding=1)\n",
    "\n",
    "        # 출력 레이어\n",
    "        self.fc = nn.Linear(embedding_dim * MAX_SEQ_LEN, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 임베딩\n",
    "        x = self.embedding(x)  # [batch_size, seq_len, embedding_dim]\n",
    "        x = x.permute(0, 2, 1)  # [batch_size, embedding_dim, seq_len]\n",
    "\n",
    "        # CNN 인코더로 압축\n",
    "        x = self.cnn_encoder(x)  # [batch_size, embedding_dim, seq_len/2]\n",
    "        x = nn.ReLU()(x)\n",
    "        x = x.permute(2, 0, 1)  # [seq_len/2, batch_size, embedding_dim]\n",
    "\n",
    "        # 트랜스포머에 전달\n",
    "        x = self.transformer_encoder(x)  # [seq_len/2, batch_size, embedding_dim]\n",
    "\n",
    "        # CNN 디코더로 복원\n",
    "        x = x.permute(1, 2, 0)  # [batch_size, embedding_dim, seq_len/2]\n",
    "        x = self.cnn_decoder(x)  # [batch_size, embedding_dim, seq_len]\n",
    "        x = nn.ReLU()(x)\n",
    "        x = x.reshape(x.size(0), -1)  # [batch_size, embedding_dim * seq_len]\n",
    "\n",
    "        # 출력 레이어\n",
    "        logits = self.fc(x)  # [batch_size, num_classes]\n",
    "        return logits\n",
    "\n",
    "# 모델 초기화\n",
    "model = CNNTransformerModel(len(vocab), EMBEDDING_DIM, HIDDEN_DIM, NUM_CLASSES)\n",
    "\n",
    "# 손실 함수와 옵티마이저 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# 모델 학습\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for texts, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(texts)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# 모델 평가\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for texts, labels in test_loader:\n",
    "        outputs = model(texts)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn-transformer-hybrid-model-fa1p7jlT-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
